{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abfdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "FOLDER_RELATIVE_PATH = \"SDS-Dev\"\n",
    "SITE_NAME = \"SafetyDataSheetManagement\"\n",
    "SHAREPOINT_HOST = \"https://envu.sharepoint.com/sites/SafetyDataSheetManagement\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2287eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get SharePoint Site and Drive IDs ---\n",
    "def get_site_and_drive_ids():\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "    # Get site info\n",
    "    site_resp = requests.get(\n",
    "        f\"https://graph.microsoft.com/v1.0/sites/envu.sharepoint.com:/sites/{SITE_NAME}\",\n",
    "        headers=headers\n",
    "    )\n",
    "    site_resp.raise_for_status()\n",
    "    site_id = site_resp.json()[\"id\"]\n",
    "\n",
    "    # Get drive info\n",
    "    drive_resp = requests.get(f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives\", headers=headers)\n",
    "    drive_resp.raise_for_status()\n",
    "    drive_id = drive_resp.json()[\"value\"][0][\"id\"]\n",
    "\n",
    "    return site_id, drive_id\n",
    "\n",
    "site_id, drive_id = get_site_and_drive_ids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "TENANT_ID = \"c4dedb74-d916-4ef4-b6b5-af80c59e9742\"\n",
    "CLIENT_ID = \"aed3878c-05a8-48ea-9de8-864434e31b23\"\n",
    "CLIENT_SECRET = \"e6k8Q~ihQPMufUTx_UV_38ivzVJK3LrvAxKMJbm9\"\n",
    "SITE_NAME = \"SafetyDataSheetManagement\"\n",
    "SHAREPOINT_HOST = \"https://envu.sharepoint.com/sites/SafetyDataSheetManagement\"\n",
    "FOLDER_RELATIVE_PATH = \"SDS-Prod\"\n",
    "\n",
    "\n",
    "def get_access_token():\n",
    "    url = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token\"\n",
    "    data = {\n",
    "        \"client_id\": CLIENT_ID,\n",
    "        \"client_secret\": CLIENT_SECRET,\n",
    "        \"scope\": \"https://graph.microsoft.com/.default\",\n",
    "        \"grant_type\": \"client_credentials\"\n",
    "    }\n",
    "    resp = requests.post(url, data=data)\n",
    "    resp.raise_for_status()\n",
    "    print(\"Access token acquired\")\n",
    "    return resp.json()[\"access_token\"]\n",
    "\n",
    "access_token = get_access_token()\n",
    "headers = {'Authorization': f'Bearer {access_token}'}\n",
    "session = requests.Session()\n",
    "\n",
    "def get_folder_item_id():\n",
    "    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives/{drive_id}/root:/{FOLDER_RELATIVE_PATH}\"\n",
    "    resp = session.get(url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"id\"]\n",
    "\n",
    "folder_id = get_folder_item_id()\n",
    "\n",
    "def fetch_all_files_recursive(drive_id, folder_id, headers, session, max_retries=5, delay=2):\n",
    "    \"\"\"\n",
    "    Recursively fetch all files from a SharePoint folder (including subfolders).\n",
    "    Handles pagination, retries, and throttling.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    # Retry strategy for robust API calls\n",
    "    retry_strategy = Retry(\n",
    "        total=max_retries,\n",
    "        backoff_factor=delay,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"https://\", adapter)\n",
    "\n",
    "    def _fetch_folder(folder_id, path=\"\"):\n",
    "        nonlocal records\n",
    "        next_url = f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/items/{folder_id}/children\"\n",
    "        while next_url:\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    resp = session.get(next_url, headers=headers, timeout=30)\n",
    "                    if resp.status_code == 429:\n",
    "                        wait_time = int(resp.headers.get(\"Retry-After\", 5))\n",
    "                        print(f\"Rate-limited. Waiting {wait_time} seconds...\")\n",
    "                        time.sleep(wait_time)\n",
    "                        continue\n",
    "                    resp.raise_for_status()\n",
    "                    data = resp.json()\n",
    "                    break\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Attempt {attempt+1} failed in folder '{path}': {e}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(delay * (attempt + 1))\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            items = data.get(\"value\", [])\n",
    "            for item in items:\n",
    "                if \"folder\" in item:  # it's a folder\n",
    "                    subfolder_name = item[\"name\"]\n",
    "                    _fetch_folder(item[\"id\"], path + \"/\" + subfolder_name)\n",
    "                else:\n",
    "                    records.append({\n",
    "                        \"id\": item.get(\"id\"),\n",
    "                        \"filename\": item.get(\"name\"),\n",
    "                        \"path\": path,\n",
    "                        \"webUrl\": item.get(\"webUrl\"),\n",
    "                        \"lastModifiedDateTime\": item.get(\"lastModifiedDateTime\"),\n",
    "                        \"size\": item.get(\"size\")\n",
    "                    })\n",
    "            next_url = data.get(\"@odata.nextLink\")\n",
    "\n",
    "        tqdm.write(f\"Fetched {len(records)} total so far from path: {path or '/'}\")\n",
    "\n",
    "    print(\"Fetching all SharePoint files (recursive)...\")\n",
    "    _fetch_folder(folder_id)\n",
    "    print(f\"\\n✅ Total files fetched: {len(records)}\")\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Run robust fetch\n",
    "sharepoint_df = fetch_all_files_recursive(drive_id, folder_id, headers, session)\n",
    "\n",
    "# Save result\n",
    "# output_excel = r\"C:\\Users\\ADMIN\\Downloads\\sds_data_combined.xlsx\"\n",
    "# sharepoint_df.to_excel(output_excel, index=False)\n",
    "# print(f\"Saved results to {output_excel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f117091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = sharepoint_df.copy()\n",
    "copy_df['md5'] = copy_df['filename'].apply(\n",
    "    lambda x: str(x).split('.')[0] if pd.notna(x) else None\n",
    ")\n",
    "copy_df = copy_df[['md5','webUrl']]\n",
    "copy_df.rename(columns = {'webUrl':'Document Url'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b31a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "server = \"bchdtawpdbeus01.database.windows.net\"\n",
    "database = \"SafetyDataSheets\"\n",
    "username = \"sds_db\"\n",
    "password  = \"Envu@3688\"\n",
    "\n",
    "query = \"\"\"\n",
    "WITH dups AS (\n",
    "    SELECT *\n",
    "    FROM sds_data\n",
    "    WHERE md5 IN (\n",
    "        SELECT md5\n",
    "        FROM sds_data\n",
    "        GROUP BY md5\n",
    "        HAVING COUNT(*) > 1\n",
    "    )\n",
    "),\n",
    "cleaned AS (\n",
    "    SELECT *,\n",
    "           CASE WHEN LOWER(filename_original) IN ('na', 'n/a', '') THEN NULL ELSE filename_original END AS filename_original_clean,\n",
    "           CASE WHEN LOWER(country_code) IN ('na', 'n/a', '') THEN NULL ELSE country_code END AS country_code_clean,\n",
    "           CASE WHEN LOWER(language_code) IN ('na', 'n/a', '') THEN NULL ELSE language_code END AS language_code_clean,\n",
    "           CASE WHEN LOWER(uvp) IN ('na', 'n/a', '') THEN NULL ELSE uvp END AS uvp_clean,\n",
    "           CASE WHEN LOWER(sku) IN ('na', 'n/a', '') THEN NULL ELSE sku END AS sku_clean,\n",
    "           CASE WHEN LOWER(version) IN ('na', 'n/a', '', '.0') THEN NULL ELSE version END AS version_clean\n",
    "    FROM dups\n",
    "),\n",
    "valid_score AS (\n",
    "    SELECT *,\n",
    "           (\n",
    "               (CASE WHEN filename_original_clean IS NOT NULL THEN 1 ELSE 0 END) +\n",
    "               (CASE WHEN country_code_clean IS NOT NULL THEN 1 ELSE 0 END) +\n",
    "               (CASE WHEN language_code_clean IS NOT NULL THEN 1 ELSE 0 END) +\n",
    "               (CASE WHEN uvp_clean IS NOT NULL THEN 1 ELSE 0 END) +\n",
    "               (CASE WHEN sku_clean IS NOT NULL THEN 1 ELSE 0 END) +\n",
    "               (CASE WHEN version_clean IS NOT NULL THEN 1 ELSE 0 END)\n",
    "           ) AS filled_columns\n",
    "    FROM cleaned\n",
    "),\n",
    "ranked AS (\n",
    "    SELECT *,\n",
    "           ROW_NUMBER() OVER (\n",
    "               PARTITION BY md5\n",
    "               ORDER BY filled_columns DESC\n",
    "           ) AS rn\n",
    "    FROM valid_score\n",
    ")\n",
    "SELECT *\n",
    "FROM ranked\n",
    "WHERE rn = 1\n",
    "\"\"\"\n",
    "conn_str = (\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    ")\n",
    "conn = pyodbc.connect(conn_str)\n",
    "df_sql_dups_cleaned = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "print(f\"Retrieved {len(df_sql_dups_cleaned)} rows from SQL Server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT *\n",
    "    FROM sds_data\n",
    "    WHERE md5 IN (\n",
    "        SELECT md5\n",
    "        FROM sds_data\n",
    "        GROUP BY md5\n",
    "        HAVING COUNT(*) > 1\n",
    "    )\n",
    "'''\n",
    "conn = pyodbc.connect(conn_str)\n",
    "duplicated_md5 = pd.read_sql(query, conn)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76803a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''select * from sds_data'''\n",
    "conn = pyodbc.connect(conn_str)\n",
    "sds_data_files = pd.read_sql(query, conn)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: remove the old duplicate rows from sds_data_files\n",
    "filtered_sds = sds_data_files[~sds_data_files['id'].isin(duplicated_md5['id'])]\n",
    "final_df = pd.concat([filtered_sds, df_sql_dups_cleaned], ignore_index=True)\n",
    "\n",
    "merged_df = final_df.merge(copy_df,on='md5',how='left')\n",
    "merged_df.to_excel('SDS-Prod.xlsx', index=False, engine='openpyxl')\n",
    "merged_df[\"Document Url\"] = '=HYPERLINK(\"' + merged_df[\"Document Url\"] + '\", \"' + merged_df[\"Document Url\"] + '\")'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f141807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sql_data(server, database, username, password):\n",
    "    conn_str = (\n",
    "        f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "        f\"SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "    )\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    df = pd.read_sql(\"SELECT * FROM dbo.sds_data\", conn)\n",
    "    conn.close()\n",
    "    print(f\"{server}: {len(df)} rows retrieved\")\n",
    "    return df\n",
    "\n",
    "df1 = fetch_sql_data(server1, database1, username1, password1)\n",
    "df2 = fetch_sql_data(server1, database2, username2, password2)\n",
    "\n",
    "\n",
    "\n",
    "# Get md5 values from Server2\n",
    "server2_md5 = set(df2[\"md5\"].dropna())\n",
    "\n",
    "# Filter Server1 to remove common md5s\n",
    "df1_unique = df1[~df1[\"md5\"].isin(server2_md5)]\n",
    "\n",
    "\n",
    "combined_sql = pd.concat([df1_unique, df2], ignore_index=True)\n",
    "print(f\"Combined SQL rows: {len(combined_sql)}\")\n",
    "\n",
    "\n",
    "\n",
    "final_df = pd.merge(combined_sql, sharepoint_df, on=\"md5\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "# final_df.to_excel(output_excel, index=False, engine=\"openpyxl\")\n",
    "print(f\"Final export completed → {output_excel}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
